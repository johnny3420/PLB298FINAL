---
title: "Genomic Prediction Workspace"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Load libraries
library(ggplot2)
library(reshape2)
library(glmnet)
library(BGLR)
library(tidyverse)
library(randomForest)
library(e1071)
```

```{r}
# Load data
load("FinalProjectData.RData")
load("CAWT_Results.RData")

# Add an empty row to phenotype for missing mother
pheno[nrow(pheno) +1, ] <- NA
pheno$Progeny[nrow(pheno)] <- "inQ5IDMeqY"

# Reorder genotype dataframe to match
G <- G[order(match(rownames(G),pheno$Progeny)),]

# Check
all.equal(rownames(G), pheno$Progeny)
dim(G)
dim(pheno)
```

```{r}
Run_MSE <- function(y = NULL, X = NULL, Train_Size = 0.8, Seed = NULL){
  
  # MSE function
  get_MSE <- function(y, yhat) mean((y-yhat)**2, na.rm=T)  # Compute MSEs
  
  mses <- c()
  Models <- c("standard lm", "lm_LASSO", "lm_RR", "lm_Balance", "B_FIXED", "B_LASSO", "B_RR",
              "RKHS", "RandomForest", "Krn_linear", "Krn_poly", "Krn_radial", "Krn_sigmoid")
  
  # Repeatability
  if(!(is.null(Seed))) set.seed(Seed)
  
  # Build Test and Train sets
  n <- length(y)
  train_idx <- sample(1:n, floor(n*Train_Size), replace=F)
  test_idx <- (1:n)[!(1:n %in% train_idx)]
  y_train <- y[train_idx]
  X_train <- X[train_idx,]
  y_test <- y[test_idx]
  X_test <- X[test_idx,]
  nas_train <- which(is.na(y_train))
  nas_test <- which(is.na(y_test))
  
  ### Run models
  
  ## LINEAR
  # Standard linear regression
  y_train_na <- y; y_train_na[test_idx] <- NA
  y_test_na <- y; y_test_na[train_idx] <- NA
  fit_l0 <- lm(y_train_na ~ X)
  yhat_test_l0 <- predict(fit_l0, newdata = data.frame(X=X, y=y))
  mses <- c(mses, round(get_MSE(y_test_na, yhat_test_l0),2))
  
  # LASSO regression
  fit_l1 <- glmnet(X_train[-nas_train,], y_train[-nas_train], alpha=1)
  yhat_test_l1 <- predict(fit_l1, X_test, s = 0.05, type = "response")
  mses <- c(mses, round(get_MSE(y_test, yhat_test_l1),2))
  
  # Ridge regression
  fit_l2 <- glmnet(X_train[-nas_train,], y_train[-nas_train], alpha=0)  
  yhat_test_l2 <- predict(fit_l1, X_test, s = 10, type = "response")
  mses <- c(mses, round(get_MSE(y_test, yhat_test_l2),2))
  
  # "Balanced" elastic net regression
  fit_l1_l2 <- glmnet(X_train[-nas_train,], y_train[-nas_train], alpha=0.5)
  yhat_test_l1_l2 <- predict(fit_l1, X_test, s = 0.05, type = "response")
  mses <- c(mses, round(get_MSE(y_test, yhat_test_l1_l2),2))
  
  ## BAYESIAN
  # Bayesian with Standard linear regression
  fit_B_l0 <- BGLR(y=y_train_na, ETA=list(list(X=X, model="FIXED")))
  yhat_test_B_l0 <- predict(fit_B_l0)
  mses <- c(mses, round(get_MSE(y_test_na, yhat_test_B_l0),2))

  # Bayesian LASSO regression
  fit_B_l1 <- BGLR(y=y_train_na, ETA=list(list(X=X, model="BL"))) 
  yhat_test_B_l1 <- predict(fit_B_l1)
  mses <- c(mses, round(get_MSE(y_test_na, yhat_test_B_l1),2))
  
  # Bayesian Ridge regression
  fit_B_l2 <- BGLR(y=y_train_na, ETA=list(list(X=X, model="BRR"))) 
  yhat_test_B_l2 <- predict(fit_B_l2)
  mses <- c(mses, round(get_MSE(y_test_na, yhat_test_B_l2),2))
  
  ## KERNALLIN
  # RKHS regression
  K <- cov(t(X))
  fit_B_KT <- BGLR(y=y_train_na, ETA=list(list(K=K, model="RKHS")))
  yhat_test_B_KT <- predict(fit_B_KT)
  mses <- c(mses, round(get_MSE(y_test_na, yhat_test_B_KT),2))
  
  ## RANDOM FOREST
  # RandomForest
  if(length(nas_test) == 0){
    fit_rf <- randomForest(x=X_train[-nas_train,], y=y_train[-nas_train], xtest=X_test, ytest=y_test, ntree=500)
  yhat_test_rf <- fit_rf$test$predicted
  mses <- c(mses, round(get_MSE(y_test[-nas_test], yhat_test_rf),2))
  } else {
  fit_rf <- randomForest(x=X_train[-nas_train,], y=y_train[-nas_train], xtest=X_test[-nas_test,], ytest=y_test[-nas_test], ntree=500)
  yhat_test_rf <- fit_rf$test$predicted
  mses <- c(mses, round(get_MSE(y_test[-nas_test], yhat_test_rf),2))
  }
  ## SVR
  # Linear kernel
  fit_svm <- svm(x=X, y=y_train_na, kernel="linear")
  yhat_test_svm <- predict(fit_svm)
  mses <- c(mses, round(get_MSE(y_test_na, yhat_test_svm),2))

  # Quadratic polynomial
  fit_svm <- svm(x=X, y=y_train_na, kernel="polynomial", degree=2)
  yhat_test_svm <- predict(fit_svm)
  mses <- c(mses, round(get_MSE(y_test_na, yhat_test_svm),2))

  # Gaussian (RBF)
  fit_svm <- svm(x = X, y = y_train_na, kernel = "radial")
  yhat_test_svm <- predict(fit_svm)
  mses <- c(mses, round(get_MSE(y_test_na, yhat_test_svm),2))
  
  # Sigmoid
  fit_svm <- svm(x = X, y = y_train_na, kernel = "sigmoid")
  yhat_test_svm <- predict(fit_svm)
  mses <- c(mses, round(get_MSE(y_test_na, yhat_test_svm),2))
  
  df <- data.frame(Model = Models, MSE = mses) %>% arrange(MSE)
  return(df)
}
```

```{r}
# y <- pheno$CAWT_WEO; X <- G[,sample(1:ncol(G), 200)]; Train_Size <- 0.8; Seed <- 554

CAWT_WEO <- Run_MSE(y = pheno$CAWT_WEO, X = G, Train_Size = 0.8, Seed = 666)
CAWT_WEO

CAWT_SAL1 <- Run_MSE(y = pheno$CAWT_SAL1, X = G, Train_Size = 0.8, Seed = 666)
CAWT_SAL1

CAWT_SAL2 <- Run_MSE(y = pheno$CAWT_SAL2, X = G, Train_Size = 0.8, Seed = 666)
CAWT_SAL2

#save(CAWT_WEO, CAWT_SAL1, CAWT_SAL2, file = "CAWT_Results.RData")
```

# Testing the 5 error messages found

1: In predict.lm(fit_l0, newdata = data.frame(X = X, y = y)) :
  prediction from a rank-deficient fit may be misleading
2: In yorig - ret$fitted :
  longer object length is not a multiple of shorter object length
3: In yorig - ret$fitted :
  longer object length is not a multiple of shorter object length
4: In yorig - ret$fitted :
  longer object length is not a multiple of shorter object length
5: In yorig - ret$fitted :
  longer object length is not a multiple of shorter object length

## lm model is absolute trash
```{r}
y <- pheno$CAWT_WEO; X <- G; Train_Size <- 0.8; Seed <- 666

  # Repeatability
  if(!(is.null(Seed))) set.seed(Seed)
  
  # Build Test and Train sets
  n <- length(y)
  train_idx <- sample(1:n, floor(n*Train_Size), replace=F)
  test_idx <- (1:n)[!(1:n %in% train_idx)]
  y_train <- y[train_idx]
  X_train <- X[train_idx,]
  y_test <- y[test_idx]
  X_test <- X[test_idx,]
  nas_train <- which(is.na(y_train))
  nas_test <- which(is.na(y_test))

  ## LINEAR
  # Standard linear regression
  y_train_na <- y; y_train_na[test_idx] <- NA
  y_test_na <- y; y_test_na[train_idx] <- NA
  fit_l0 <- lm(y_train_na ~ X)
  yhat_test_l0 <- predict(fit_l0, newdata = data.frame(X=X, y=y))
  round(get_MSE(y_test_na, yhat_test_l0),2) # 840457435
  
## Duplicate markers
SNP_profiles <- data.frame(Marker = colnames(G), Contents = NA)
for(i in 1:nrow(SNP_profiles)){
  SNP_profiles$Contents[i] <- paste(G[,i,drop = T], collapse = "")
}
SNP_profiles <- SNP_profiles %>%
  mutate(Contents = as.integer(as.factor(Contents)))

sum(duplicated(SNP_profiles$Contents)) # 1237 markers which are identical

SNP_profiles %>% 
  mutate(Contents = as.factor(Contents)) %>%
  count(Contents) %>%
  filter(n > 2) %>%
  arrange(-n)

### What happens if I remove the duplicated markers
Markers2Keep <- SNP_profiles %>%
  group_by(Contents) %>%
  slice(1) %>%
  pull(Marker)
Markers2Keep <- as.character(Markers2Keep)
G2 <- G[,Markers2Keep]

y <- pheno$CAWT_WEO; X <- G2; Train_Size <- 0.8; Seed <- 666

  # Repeatability
  if(!(is.null(Seed))) set.seed(Seed)
  
  # Build Test and Train sets
  n <- length(y)
  train_idx <- sample(1:n, floor(n*Train_Size), replace=F)
  test_idx <- (1:n)[!(1:n %in% train_idx)]
  y_train <- y[train_idx]
  X_train <- X[train_idx,]
  y_test <- y[test_idx]
  X_test <- X[test_idx,]
  nas_train <- which(is.na(y_train))
  nas_test <- which(is.na(y_test))

  ## LINEAR
  # Standard linear regression
  y_train_na <- y; y_train_na[test_idx] <- NA
  y_test_na <- y; y_test_na[train_idx] <- NA
  fit_l0 <- lm(y_train_na ~ X)
  yhat_test_l0 <- predict(fit_l0, newdata = data.frame(X=X, y=y))
  round(get_MSE(y_test_na, yhat_test_l0),2) #281979763
  
### MSE is ~3X larger in the lm model if you leave the duplicates in...
  
### Add marker positions in
markers <- read_delim("markers.txt", delim = " ", col_names = c("chromosome","position","marker","ref","alt"))
SNP_profiles <- SNP_profiles %>%
  left_join(markers %>%
              select(marker,chromosome,position), by = c("Marker" = "marker"))

for(i in 1:nrow(SNP_profiles)){
  SNP_profiles$Contents[i] <- paste0(SNP_profiles$chromosome[i],paste(G[,i,drop = T], collapse = ""))
}
SNP_profiles <- SNP_profiles %>%
  mutate(Contents = as.integer(as.factor(Contents)))

sum(duplicated(SNP_profiles$Contents)) # 1129 markers which are identical

SNP_profiles %>% 
  mutate(Contents = as.factor(Contents)) %>%
  count(Contents) %>%
  filter(n >= 2) %>%
  arrange(-n)

Markers2Keep <- SNP_profiles %>%
  group_by(Contents) %>%
  slice(1) %>%
  pull(Marker)
Markers2Keep <- as.character(Markers2Keep)
G2 <- G[,Markers2Keep]

y <- pheno$CAWT_WEO; X <- G2; Train_Size <- 0.8; Seed <- 666

  # Repeatability
  if(!(is.null(Seed))) set.seed(Seed)
  
  # Build Test and Train sets
  n <- length(y)
  train_idx <- sample(1:n, floor(n*Train_Size), replace=F)
  test_idx <- (1:n)[!(1:n %in% train_idx)]
  y_train <- y[train_idx]
  X_train <- X[train_idx,]
  y_test <- y[test_idx]
  X_test <- X[test_idx,]
  nas_train <- which(is.na(y_train))
  nas_test <- which(is.na(y_test))

  ## LINEAR
  # Standard linear regression
  y_train_na <- y; y_train_na[test_idx] <- NA
  y_test_na <- y; y_test_na[train_idx] <- NA
  fit_l0 <- lm(y_train_na ~ X)
  yhat_test_l0 <- predict(fit_l0, newdata = data.frame(X=X, y=y))
  round(get_MSE(y_test_na, yhat_test_l0),2) # 14999661979
  
  14999661979/840457435
  
  ## ~18X worse if I only keep one marker from each chrom_marker duplicate set
```

## Other 4 messages

```{r}
# Quadratic polynomial
nas_train <- which(is.na(y))
  fit_svm <- svm(x=X[-nas_train,], y=y[-nas_train], kernel="polynomial", degree=2)
  yhat_test_svm <- predict(fit_svm)
  round(get_MSE(y[-nas_train], yhat_test_svm),2)

# Can be ignored due to have NAs in the training set. Wants to compare to yorig to fitted, but yorig is NA making the list shorter
```

### Use year one to predict year two

```{r}
year_1 <- pheno$CAWT_SAL1
year_2 <- pheno$CAWT_SAL2
seed <- 666
# can drop unphenotyped mother
year_1 <- year_1[-571]
year_2 <- year_2[-571]
X <- G[-571,]

# MSE function
get_MSE <-
  function(y, yhat)
    mean((y - yhat) ** 2, na.rm = T)  # Compute MSEs

mses <- c()
Models <-
  c(
    "standard lm",
    "lm_LASSO",
    "lm_RR",
    "lm_Balance",
    "B_FIXED",
    "B_LASSO",
    "B_RR",
    "RKHS",
    "RandomForest",
    "Krn_linear",
    "Krn_poly",
    "Krn_radial",
    "Krn_sigmoid"
  )

# Repeatability
if (!(is.null(Seed)))
  set.seed(Seed)

# Build Train/Test set
y_train <- year_1
y_test <- year_2

### Run models

## LINEAR
    # Standard linear regression
fit_l0 <- lm(y_train ~ X)
yhat_test_l0 <- predict(fit_l0, newdata = data.frame(X = X, y = y_test))
mses <- c(mses, round(get_MSE(y_test, yhat_test_l0), 2))

     # LASSO regression
fit_l1 <- glmnet(X, y_train, alpha = 1)
yhat_test_l1 <- predict(fit_l1, X, s = 0.05, type = "response")
mses <- c(mses, round(get_MSE(y_test, yhat_test_l1), 2))

# Ridge regression
fit_l2 <- glmnet(X, y_train, alpha = 0)
yhat_test_l2 <- predict(fit_l1, X, s = 10, type = "response")
mses <- c(mses, round(get_MSE(y_test, yhat_test_l2), 2))

# "Balanced" elastic net regression
fit_l1_l2 <- glmnet(X, y_train, alpha = 0.5)
yhat_test_l1_l2 <- predict(fit_l1, X, s = 0.05, type = "response")
mses <- c(mses, round(get_MSE(y_test, yhat_test_l1_l2), 2))

## BAYESIAN
# Bayesian with Standard linear regression
fit_B_l0 <- BGLR(y = y_train, ETA = list(list(X = X, model = "FIXED")))
yhat_test_B_l0 <- predict(fit_B_l0)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_l0), 2))

# Bayesian LASSO regression
fit_B_l1 <- BGLR(y = y_train, ETA = list(list(X = X, model = "BL")))
yhat_test_B_l1 <- predict(fit_B_l1)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_l1), 2))

# Bayesian Ridge regression
fit_B_l2 <- BGLR(y = y_train, ETA = list(list(X = X, model = "BRR")))
yhat_test_B_l2 <- predict(fit_B_l2)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_l2), 2))

## KERNALLIN
# RKHS regression
K <- cov(t(X))
fit_B_KT <- BGLR(y = y_train, ETA = list(list(K = K, model = "RKHS")))
yhat_test_B_KT <- predict(fit_B_KT)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_KT), 2))

## RANDOM FOREST
# RandomForest
test_nas <- which(is.na(y_test))
fit_rf <-
  randomForest(
    x = X,
    y = y,
    xtest = X[-test_nas,],
    ytest = y_test[-test_nas],
    ntree = 500
  )
yhat_test_rf <- fit_rf$test$predicted
mses <- c(mses, round(get_MSE(y_test[-test_nas], yhat_test_rf), 2))

## SVR
# Linear kernel
fit_svm <- svm(x = X, y = y_train, kernel = "linear")
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

# Quadratic polynomial
fit_svm <- svm(x = X, y = y_train, kernel = "polynomial", degree = 2)
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

# Gaussian (RBF)
fit_svm <- svm(x = X, y = y_train, kernel = "radial")
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

# Sigmoid
fit_svm <- svm(x = X, y = y_train, kernel = "sigmoid")
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

Year2Year <- data.frame(Model = Models, MSE = mses) %>% arrange(MSE)
#save(CAWT_SAL1,CAWT_SAL2,CAWT_WEO,Year2Year, file = "CAWT_Results.RData")

Year2Year
hist(pheno$CAWT_SAL2)

```

## Run it all
```{r}
CAWT_WEO <- Run_MSE(y = pheno$CAWT_WEO, X = G, Train_Size = 0.8, Seed = 666)
CAWT_WEO

CAWT_SAL1 <- Run_MSE(y = pheno$CAWT_SAL1, X = G, Train_Size = 0.8, Seed = 666)
CAWT_SAL1

CAWT_SAL2 <- Run_MSE(y = pheno$CAWT_SAL2, X = G, Train_Size = 0.8, Seed = 666)
CAWT_SAL2

CCT_WEO <- Run_MSE(y = pheno$CCT_WEO, X = G, Train_Size = 0.8, Seed = 666)
CCT_WEO

CCT_SAL1 <- Run_MSE(y = pheno$CCT_SAL1, X = G, Train_Size = 0.8, Seed = 666)
CCT_SAL1

CCT_SAL2 <- Run_MSE(y = pheno$CCT_SAL2, X = G, Train_Size = 0.8, Seed = 666)
CCT_SAL2

CWT_WEO <- Run_MSE(y = pheno$CWT_WEO, X = G, Train_Size = 0.8, Seed = 666)
CWT_WEO

CWT_SAL1 <- Run_MSE(y = pheno$CWT_SAL1, X = G, Train_Size = 0.8, Seed = 666)
CWT_SAL1

CWT_SAL2 <- Run_MSE(y = pheno$CWT_SAL2, X = G, Train_Size = 0.8, Seed = 666)
CWT_SAL2
```

```{r}
#Year 1 vs Year 2 CAWT SAL
year_1 <- pheno$CAWT_SAL1
year_2 <- pheno$CAWT_SAL2
seed <- 666
# can drop unphenotyped mother
year_1 <- year_1[-571]
year_2 <- year_2[-571]
X <- G[-571,]

# MSE function
get_MSE <-
  function(y, yhat)
    mean((y - yhat) ** 2, na.rm = T)  # Compute MSEs

mses <- c()
Models <-
  c(
    "standard lm",
    "lm_LASSO",
    "lm_RR",
    "lm_Balance",
    "B_FIXED",
    "B_LASSO",
    "B_RR",
    "RKHS",
    "RandomForest",
    "Krn_linear",
    "Krn_poly",
    "Krn_radial",
    "Krn_sigmoid"
  )

# Repeatability
if (!(is.null(Seed)))
  set.seed(Seed)

# Build Train/Test set
y_train <- year_1
y_test <- year_2

### Run models

## LINEAR
    # Standard linear regression
fit_l0 <- lm(y_train ~ X)
yhat_test_l0 <- predict(fit_l0, newdata = data.frame(X = X, y = y_test))
mses <- c(mses, round(get_MSE(y_test, yhat_test_l0), 2))

     # LASSO regression
fit_l1 <- glmnet(X, y_train, alpha = 1)
yhat_test_l1 <- predict(fit_l1, X, s = 0.05, type = "response")
mses <- c(mses, round(get_MSE(y_test, yhat_test_l1), 2))

# Ridge regression
fit_l2 <- glmnet(X, y_train, alpha = 0)
yhat_test_l2 <- predict(fit_l1, X, s = 10, type = "response")
mses <- c(mses, round(get_MSE(y_test, yhat_test_l2), 2))

# "Balanced" elastic net regression
fit_l1_l2 <- glmnet(X, y_train, alpha = 0.5)
yhat_test_l1_l2 <- predict(fit_l1, X, s = 0.05, type = "response")
mses <- c(mses, round(get_MSE(y_test, yhat_test_l1_l2), 2))

## BAYESIAN
# Bayesian with Standard linear regression
fit_B_l0 <- BGLR(y = y_train, ETA = list(list(X = X, model = "FIXED")))
yhat_test_B_l0 <- predict(fit_B_l0)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_l0), 2))

# Bayesian LASSO regression
fit_B_l1 <- BGLR(y = y_train, ETA = list(list(X = X, model = "BL")))
yhat_test_B_l1 <- predict(fit_B_l1)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_l1), 2))

# Bayesian Ridge regression
fit_B_l2 <- BGLR(y = y_train, ETA = list(list(X = X, model = "BRR")))
yhat_test_B_l2 <- predict(fit_B_l2)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_l2), 2))

## KERNALLIN
# RKHS regression
K <- cov(t(X))
fit_B_KT <- BGLR(y = y_train, ETA = list(list(K = K, model = "RKHS")))
yhat_test_B_KT <- predict(fit_B_KT)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_KT), 2))

## RANDOM FOREST
# RandomForest
test_nas <- which(is.na(y_test))
fit_rf <-
  randomForest(
    x = X,
    y = y,
    xtest = X[-test_nas,],
    ytest = y_test[-test_nas],
    ntree = 500
  )
yhat_test_rf <- fit_rf$test$predicted
mses <- c(mses, round(get_MSE(y_test[-test_nas], yhat_test_rf), 2))

## SVR
# Linear kernel
fit_svm <- svm(x = X, y = y_train, kernel = "linear")
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

# Quadratic polynomial
fit_svm <- svm(x = X, y = y_train, kernel = "polynomial", degree = 2)
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

# Gaussian (RBF)
fit_svm <- svm(x = X, y = y_train, kernel = "radial")
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

# Sigmoid
fit_svm <- svm(x = X, y = y_train, kernel = "sigmoid")
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

Year2Year <- data.frame(Model = Models, MSE = mses) %>% arrange(MSE)
#save(CAWT_SAL1,CAWT_SAL2,CAWT_WEO,Year2Year, file = "CAWT_Results.RData")
```

```{r}
#Year 1 vs Year 2 CAWT WEO
year_1 <- pheno$CAWT_WEO
year_2 <- pheno$CAWT_SAL2
seed <- 666
# can drop unphenotyped mother
year_1 <- year_1[-571]
year_2 <- year_2[-571]
X <- G[-571,]

# MSE function
get_MSE <-
  function(y, yhat)
    mean((y - yhat) ** 2, na.rm = T)  # Compute MSEs

mses <- c()
Models <-
  c(
    "standard lm",
    "lm_LASSO",
    "lm_RR",
    "lm_Balance",
    "B_FIXED",
    "B_LASSO",
    "B_RR",
    "RKHS",
    "RandomForest",
    "Krn_linear",
    "Krn_poly",
    "Krn_radial",
    "Krn_sigmoid"
  )

# Repeatability
if (!(is.null(Seed)))
  set.seed(Seed)

# Build Train/Test set
y_train <- year_1
y_test <- year_2

### Run models

## LINEAR
    # Standard linear regression
fit_l0 <- lm(y_train ~ X)
yhat_test_l0 <- predict(fit_l0, newdata = data.frame(X = X, y = y_test))
mses <- c(mses, round(get_MSE(y_test, yhat_test_l0), 2))

     # LASSO regression
fit_l1 <- glmnet(X, y_train, alpha = 1)
yhat_test_l1 <- predict(fit_l1, X, s = 0.05, type = "response")
mses <- c(mses, round(get_MSE(y_test, yhat_test_l1), 2))

# Ridge regression
fit_l2 <- glmnet(X, y_train, alpha = 0)
yhat_test_l2 <- predict(fit_l1, X, s = 10, type = "response")
mses <- c(mses, round(get_MSE(y_test, yhat_test_l2), 2))

# "Balanced" elastic net regression
fit_l1_l2 <- glmnet(X, y_train, alpha = 0.5)
yhat_test_l1_l2 <- predict(fit_l1, X, s = 0.05, type = "response")
mses <- c(mses, round(get_MSE(y_test, yhat_test_l1_l2), 2))

## BAYESIAN
# Bayesian with Standard linear regression
fit_B_l0 <- BGLR(y = y_train, ETA = list(list(X = X, model = "FIXED")))
yhat_test_B_l0 <- predict(fit_B_l0)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_l0), 2))

# Bayesian LASSO regression
fit_B_l1 <- BGLR(y = y_train, ETA = list(list(X = X, model = "BL")))
yhat_test_B_l1 <- predict(fit_B_l1)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_l1), 2))

# Bayesian Ridge regression
fit_B_l2 <- BGLR(y = y_train, ETA = list(list(X = X, model = "BRR")))
yhat_test_B_l2 <- predict(fit_B_l2)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_l2), 2))

## KERNALLIN
# RKHS regression
K <- cov(t(X))
fit_B_KT <- BGLR(y = y_train, ETA = list(list(K = K, model = "RKHS")))
yhat_test_B_KT <- predict(fit_B_KT)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_KT), 2))

## RANDOM FOREST
# RandomForest
test_nas <- which(is.na(y_test))
fit_rf <-
  randomForest(
    x = X,
    y = y,
    xtest = X[-test_nas,],
    ytest = y_test[-test_nas],
    ntree = 500
  )
yhat_test_rf <- fit_rf$test$predicted
mses <- c(mses, round(get_MSE(y_test[-test_nas], yhat_test_rf), 2))

## SVR
# Linear kernel
fit_svm <- svm(x = X, y = y_train, kernel = "linear")
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

# Quadratic polynomial
fit_svm <- svm(x = X, y = y_train, kernel = "polynomial", degree = 2)
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

# Gaussian (RBF)
fit_svm <- svm(x = X, y = y_train, kernel = "radial")
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

# Sigmoid
fit_svm <- svm(x = X, y = y_train, kernel = "sigmoid")
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

Year2Year <- data.frame(Model = Models, MSE = mses) %>% arrange(MSE)
#save(CAWT_SAL1,CAWT_SAL2,CAWT_WEO,Year2Year, file = "CAWT_Results.RData")
```

```{r}
#Year 1 vs Year 2 CCT SAL
year_1 <- pheno$CCT_SAL1
year_2 <- pheno$CCT_SAL2
seed <- 666
# can drop unphenotyped mother
year_1 <- year_1[-571]
year_2 <- year_2[-571]
X <- G[-571,]

# MSE function
get_MSE <-
  function(y, yhat)
    mean((y - yhat) ** 2, na.rm = T)  # Compute MSEs

mses <- c()
Models <-
  c(
    "standard lm",
    "lm_LASSO",
    "lm_RR",
    "lm_Balance",
    "B_FIXED",
    "B_LASSO",
    "B_RR",
    "RKHS",
    "RandomForest",
    "Krn_linear",
    "Krn_poly",
    "Krn_radial",
    "Krn_sigmoid"
  )

# Repeatability
if (!(is.null(Seed)))
  set.seed(Seed)

# Build Train/Test set
y_train <- year_1
y_test <- year_2

### Run models

## LINEAR
    # Standard linear regression
fit_l0 <- lm(y_train ~ X)
yhat_test_l0 <- predict(fit_l0, newdata = data.frame(X = X, y = y_test))
mses <- c(mses, round(get_MSE(y_test, yhat_test_l0), 2))

     # LASSO regression
fit_l1 <- glmnet(X, y_train, alpha = 1)
yhat_test_l1 <- predict(fit_l1, X, s = 0.05, type = "response")
mses <- c(mses, round(get_MSE(y_test, yhat_test_l1), 2))

# Ridge regression
fit_l2 <- glmnet(X, y_train, alpha = 0)
yhat_test_l2 <- predict(fit_l1, X, s = 10, type = "response")
mses <- c(mses, round(get_MSE(y_test, yhat_test_l2), 2))

# "Balanced" elastic net regression
fit_l1_l2 <- glmnet(X, y_train, alpha = 0.5)
yhat_test_l1_l2 <- predict(fit_l1, X, s = 0.05, type = "response")
mses <- c(mses, round(get_MSE(y_test, yhat_test_l1_l2), 2))

## BAYESIAN
# Bayesian with Standard linear regression
fit_B_l0 <- BGLR(y = y_train, ETA = list(list(X = X, model = "FIXED")))
yhat_test_B_l0 <- predict(fit_B_l0)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_l0), 2))

# Bayesian LASSO regression
fit_B_l1 <- BGLR(y = y_train, ETA = list(list(X = X, model = "BL")))
yhat_test_B_l1 <- predict(fit_B_l1)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_l1), 2))

# Bayesian Ridge regression
fit_B_l2 <- BGLR(y = y_train, ETA = list(list(X = X, model = "BRR")))
yhat_test_B_l2 <- predict(fit_B_l2)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_l2), 2))

## KERNALLIN
# RKHS regression
K <- cov(t(X))
fit_B_KT <- BGLR(y = y_train, ETA = list(list(K = K, model = "RKHS")))
yhat_test_B_KT <- predict(fit_B_KT)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_KT), 2))

## RANDOM FOREST
# RandomForest
test_nas <- which(is.na(y_test))
fit_rf <-
  randomForest(
    x = X,
    y = y,
    xtest = X[-test_nas,],
    ytest = y_test[-test_nas],
    ntree = 500
  )
yhat_test_rf <- fit_rf$test$predicted
mses <- c(mses, round(get_MSE(y_test[-test_nas], yhat_test_rf), 2))

## SVR
# Linear kernel
fit_svm <- svm(x = X, y = y_train, kernel = "linear")
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

# Quadratic polynomial
fit_svm <- svm(x = X, y = y_train, kernel = "polynomial", degree = 2)
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

# Gaussian (RBF)
fit_svm <- svm(x = X, y = y_train, kernel = "radial")
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

# Sigmoid
fit_svm <- svm(x = X, y = y_train, kernel = "sigmoid")
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

Year2Year <- data.frame(Model = Models, MSE = mses) %>% arrange(MSE)
#save(CAWT_SAL1,CAWT_SAL2,CAWT_WEO,Year2Year, file = "CAWT_Results.RData")
```

```{r}
#Year 1 vs Year 2 CCT WEO
year_1 <- pheno$CCT_WEO
year_2 <- pheno$CCT_SAL2
seed <- 666
# can drop unphenotyped mother
year_1 <- year_1[-571]
year_2 <- year_2[-571]
X <- G[-571,]

# MSE function
get_MSE <-
  function(y, yhat)
    mean((y - yhat) ** 2, na.rm = T)  # Compute MSEs

mses <- c()
Models <-
  c(
    "standard lm",
    "lm_LASSO",
    "lm_RR",
    "lm_Balance",
    "B_FIXED",
    "B_LASSO",
    "B_RR",
    "RKHS",
    "RandomForest",
    "Krn_linear",
    "Krn_poly",
    "Krn_radial",
    "Krn_sigmoid"
  )

# Repeatability
if (!(is.null(Seed)))
  set.seed(Seed)

# Build Train/Test set
y_train <- year_1
y_test <- year_2

### Run models

## LINEAR
    # Standard linear regression
fit_l0 <- lm(y_train ~ X)
yhat_test_l0 <- predict(fit_l0, newdata = data.frame(X = X, y = y_test))
mses <- c(mses, round(get_MSE(y_test, yhat_test_l0), 2))

     # LASSO regression
fit_l1 <- glmnet(X, y_train, alpha = 1)
yhat_test_l1 <- predict(fit_l1, X, s = 0.05, type = "response")
mses <- c(mses, round(get_MSE(y_test, yhat_test_l1), 2))

# Ridge regression
fit_l2 <- glmnet(X, y_train, alpha = 0)
yhat_test_l2 <- predict(fit_l1, X, s = 10, type = "response")
mses <- c(mses, round(get_MSE(y_test, yhat_test_l2), 2))

# "Balanced" elastic net regression
fit_l1_l2 <- glmnet(X, y_train, alpha = 0.5)
yhat_test_l1_l2 <- predict(fit_l1, X, s = 0.05, type = "response")
mses <- c(mses, round(get_MSE(y_test, yhat_test_l1_l2), 2))

## BAYESIAN
# Bayesian with Standard linear regression
fit_B_l0 <- BGLR(y = y_train, ETA = list(list(X = X, model = "FIXED")))
yhat_test_B_l0 <- predict(fit_B_l0)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_l0), 2))

# Bayesian LASSO regression
fit_B_l1 <- BGLR(y = y_train, ETA = list(list(X = X, model = "BL")))
yhat_test_B_l1 <- predict(fit_B_l1)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_l1), 2))

# Bayesian Ridge regression
fit_B_l2 <- BGLR(y = y_train, ETA = list(list(X = X, model = "BRR")))
yhat_test_B_l2 <- predict(fit_B_l2)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_l2), 2))

## KERNALLIN
# RKHS regression
K <- cov(t(X))
fit_B_KT <- BGLR(y = y_train, ETA = list(list(K = K, model = "RKHS")))
yhat_test_B_KT <- predict(fit_B_KT)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_KT), 2))

## RANDOM FOREST
# RandomForest
test_nas <- which(is.na(y_test))
fit_rf <-
  randomForest(
    x = X,
    y = y,
    xtest = X[-test_nas,],
    ytest = y_test[-test_nas],
    ntree = 500
  )
yhat_test_rf <- fit_rf$test$predicted
mses <- c(mses, round(get_MSE(y_test[-test_nas], yhat_test_rf), 2))

## SVR
# Linear kernel
fit_svm <- svm(x = X, y = y_train, kernel = "linear")
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

# Quadratic polynomial
fit_svm <- svm(x = X, y = y_train, kernel = "polynomial", degree = 2)
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

# Gaussian (RBF)
fit_svm <- svm(x = X, y = y_train, kernel = "radial")
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

# Sigmoid
fit_svm <- svm(x = X, y = y_train, kernel = "sigmoid")
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

Year2Year <- data.frame(Model = Models, MSE = mses) %>% arrange(MSE)
#save(CAWT_SAL1,CAWT_SAL2,CAWT_WEO,Year2Year, file = "CAWT_Results.RData")
```

```{r}
#Year 1 vs Year 2 CWT SAL
year_1 <- pheno$CWT_SAL1
year_2 <- pheno$CWT_SAL2
seed <- 666
# can drop unphenotyped mother
year_1 <- year_1[-571]
year_2 <- year_2[-571]
X <- G[-571,]

# MSE function
get_MSE <-
  function(y, yhat)
    mean((y - yhat) ** 2, na.rm = T)  # Compute MSEs

mses <- c()
Models <-
  c(
    "standard lm",
    "lm_LASSO",
    "lm_RR",
    "lm_Balance",
    "B_FIXED",
    "B_LASSO",
    "B_RR",
    "RKHS",
    "RandomForest",
    "Krn_linear",
    "Krn_poly",
    "Krn_radial",
    "Krn_sigmoid"
  )

# Repeatability
if (!(is.null(Seed)))
  set.seed(Seed)

# Build Train/Test set
y_train <- year_1
y_test <- year_2

### Run models

## LINEAR
    # Standard linear regression
fit_l0 <- lm(y_train ~ X)
yhat_test_l0 <- predict(fit_l0, newdata = data.frame(X = X, y = y_test))
mses <- c(mses, round(get_MSE(y_test, yhat_test_l0), 2))

     # LASSO regression
fit_l1 <- glmnet(X, y_train, alpha = 1)
yhat_test_l1 <- predict(fit_l1, X, s = 0.05, type = "response")
mses <- c(mses, round(get_MSE(y_test, yhat_test_l1), 2))

# Ridge regression
fit_l2 <- glmnet(X, y_train, alpha = 0)
yhat_test_l2 <- predict(fit_l1, X, s = 10, type = "response")
mses <- c(mses, round(get_MSE(y_test, yhat_test_l2), 2))

# "Balanced" elastic net regression
fit_l1_l2 <- glmnet(X, y_train, alpha = 0.5)
yhat_test_l1_l2 <- predict(fit_l1, X, s = 0.05, type = "response")
mses <- c(mses, round(get_MSE(y_test, yhat_test_l1_l2), 2))

## BAYESIAN
# Bayesian with Standard linear regression
fit_B_l0 <- BGLR(y = y_train, ETA = list(list(X = X, model = "FIXED")))
yhat_test_B_l0 <- predict(fit_B_l0)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_l0), 2))

# Bayesian LASSO regression
fit_B_l1 <- BGLR(y = y_train, ETA = list(list(X = X, model = "BL")))
yhat_test_B_l1 <- predict(fit_B_l1)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_l1), 2))

# Bayesian Ridge regression
fit_B_l2 <- BGLR(y = y_train, ETA = list(list(X = X, model = "BRR")))
yhat_test_B_l2 <- predict(fit_B_l2)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_l2), 2))

## KERNALLIN
# RKHS regression
K <- cov(t(X))
fit_B_KT <- BGLR(y = y_train, ETA = list(list(K = K, model = "RKHS")))
yhat_test_B_KT <- predict(fit_B_KT)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_KT), 2))

## RANDOM FOREST
# RandomForest
test_nas <- which(is.na(y_test))
fit_rf <-
  randomForest(
    x = X,
    y = y,
    xtest = X[-test_nas,],
    ytest = y_test[-test_nas],
    ntree = 500
  )
yhat_test_rf <- fit_rf$test$predicted
mses <- c(mses, round(get_MSE(y_test[-test_nas], yhat_test_rf), 2))

## SVR
# Linear kernel
fit_svm <- svm(x = X, y = y_train, kernel = "linear")
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

# Quadratic polynomial
fit_svm <- svm(x = X, y = y_train, kernel = "polynomial", degree = 2)
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

# Gaussian (RBF)
fit_svm <- svm(x = X, y = y_train, kernel = "radial")
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

# Sigmoid
fit_svm <- svm(x = X, y = y_train, kernel = "sigmoid")
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

Year2Year <- data.frame(Model = Models, MSE = mses) %>% arrange(MSE)
#save(CAWT_SAL1,CAWT_SAL2,CAWT_WEO,Year2Year, file = "CAWT_Results.RData")
```

```{r}
#Year 1 vs Year 2 CWT WEO
year_1 <- pheno$CWT_WEO
year_2 <- pheno$CWT_SAL2
seed <- 666
# can drop unphenotyped mother
year_1 <- year_1[-571]
year_2 <- year_2[-571]
X <- G[-571,]

# MSE function
get_MSE <-
  function(y, yhat)
    mean((y - yhat) ** 2, na.rm = T)  # Compute MSEs

mses <- c()
Models <-
  c(
    "standard lm",
    "lm_LASSO",
    "lm_RR",
    "lm_Balance",
    "B_FIXED",
    "B_LASSO",
    "B_RR",
    "RKHS",
    "RandomForest",
    "Krn_linear",
    "Krn_poly",
    "Krn_radial",
    "Krn_sigmoid"
  )

# Repeatability
if (!(is.null(Seed)))
  set.seed(Seed)

# Build Train/Test set
y_train <- year_1
y_test <- year_2

### Run models

## LINEAR
    # Standard linear regression
fit_l0 <- lm(y_train ~ X)
yhat_test_l0 <- predict(fit_l0, newdata = data.frame(X = X, y = y_test))
mses <- c(mses, round(get_MSE(y_test, yhat_test_l0), 2))

     # LASSO regression
fit_l1 <- glmnet(X, y_train, alpha = 1)
yhat_test_l1 <- predict(fit_l1, X, s = 0.05, type = "response")
mses <- c(mses, round(get_MSE(y_test, yhat_test_l1), 2))

# Ridge regression
fit_l2 <- glmnet(X, y_train, alpha = 0)
yhat_test_l2 <- predict(fit_l1, X, s = 10, type = "response")
mses <- c(mses, round(get_MSE(y_test, yhat_test_l2), 2))

# "Balanced" elastic net regression
fit_l1_l2 <- glmnet(X, y_train, alpha = 0.5)
yhat_test_l1_l2 <- predict(fit_l1, X, s = 0.05, type = "response")
mses <- c(mses, round(get_MSE(y_test, yhat_test_l1_l2), 2))

## BAYESIAN
# Bayesian with Standard linear regression
fit_B_l0 <- BGLR(y = y_train, ETA = list(list(X = X, model = "FIXED")))
yhat_test_B_l0 <- predict(fit_B_l0)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_l0), 2))

# Bayesian LASSO regression
fit_B_l1 <- BGLR(y = y_train, ETA = list(list(X = X, model = "BL")))
yhat_test_B_l1 <- predict(fit_B_l1)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_l1), 2))

# Bayesian Ridge regression
fit_B_l2 <- BGLR(y = y_train, ETA = list(list(X = X, model = "BRR")))
yhat_test_B_l2 <- predict(fit_B_l2)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_l2), 2))

## KERNALLIN
# RKHS regression
K <- cov(t(X))
fit_B_KT <- BGLR(y = y_train, ETA = list(list(K = K, model = "RKHS")))
yhat_test_B_KT <- predict(fit_B_KT)
mses <- c(mses, round(get_MSE(y_test, yhat_test_B_KT), 2))

## RANDOM FOREST
# RandomForest
test_nas <- which(is.na(y_test))
fit_rf <-
  randomForest(
    x = X,
    y = y,
    xtest = X[-test_nas,],
    ytest = y_test[-test_nas],
    ntree = 500
  )
yhat_test_rf <- fit_rf$test$predicted
mses <- c(mses, round(get_MSE(y_test[-test_nas], yhat_test_rf), 2))

## SVR
# Linear kernel
fit_svm <- svm(x = X, y = y_train, kernel = "linear")
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

# Quadratic polynomial
fit_svm <- svm(x = X, y = y_train, kernel = "polynomial", degree = 2)
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

# Gaussian (RBF)
fit_svm <- svm(x = X, y = y_train, kernel = "radial")
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

# Sigmoid
fit_svm <- svm(x = X, y = y_train, kernel = "sigmoid")
yhat_test_svm <- predict(fit_svm)
mses <- c(mses, round(get_MSE(y_test, yhat_test_svm), 2))

Year2Year <- data.frame(Model = Models, MSE = mses) %>% arrange(MSE)
#save(CAWT_SAL1,CAWT_SAL2,CAWT_WEO,Year2Year, file = "CAWT_Results.RData")
```
